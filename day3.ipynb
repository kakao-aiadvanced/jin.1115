{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "H_q9dvbeXMPk"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain_community langchainhub chromadb langchain langgraph tavily-python langchain-text-splitters langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature = 0)\n",
        "\n",
        "from tavily import TavilyClient\n",
        "tavily = TavilyClient(api_key='')"
      ],
      "metadata": {
        "id": "ybgH2DoCXnDQ"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Index\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=250, chunk_overlap=0\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "# Add to vectorDB\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag-chroma\",\n",
        "    embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYE2AYw2X-l0",
        "outputId": "68780009-ec1f-4063-d822-0f99009a27de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"what is memory agent ?\""
      ],
      "metadata": {
        "id": "ENtOx5o3Ylaq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Docs Retrieval\n",
        "\n",
        "docs = retriever.get_relevant_documents(user_query)\n",
        "\n",
        "# print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrFYcZ7PYYrl",
        "outputId": "362697eb-fde0-4889-b894-c834066de04b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-7-2066480913.py:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = retriever.get_relevant_documents(user_query)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Relevance Checker\n",
        "\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "relevance_checker_system = \"\"\"\n",
        "당신은 AI 기반 문서 연관성 평가자입니다.\n",
        "\n",
        "사용자의 질문과 검색된 문서 조각(context)을 참고하여, 이 문서가 질문에 **직접적인 관련이 있는지** 판단하세요.\n",
        "- 관련성이 높다면 relevant_score를 높은 점수로 설정하세요.\n",
        "- 관련성이 낮다면 relevant_score를 낮은 점수로 설정하세요.\n",
        "- relevant_score의 범위는 0 ~ 100 사이로 설정하세요.\n",
        "\n",
        "추측하지 말고, 문서 안에 **질문에 답할 수 있는 정보가 실제로 존재**할 때만 true를 반환하세요.\n",
        "\n",
        "질문:\n",
        "{question}\n",
        "\n",
        "문서 조각:\n",
        "{document}\n",
        "\n",
        "결과는 다음 형식의 JSON으로 반환하세요. 대문자는 사용하지 마세요.\n",
        "{{\n",
        "  \"relevant_score\": 0 ~ 100\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "relevance_checker_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", relevance_checker_system),\n",
        "        (\"human\", \"question: {question}\\n\\n document: {document}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "relevance_checker = relevance_checker_prompt | llm | JsonOutputParser()\n",
        "docs = retriever.invoke(user_query)\n",
        "doc_txt = docs[0].page_content\n",
        "relevance_checker_result = relevance_checker.invoke({\"question\": user_query, \"document\": doc_txt})\n"
      ],
      "metadata": {
        "id": "MwvujlLZYwBF"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"relevance_check_result :\", relevance_checker_result['relevant_score'])\n",
        "\n",
        "if int(relevance_checker_result['relevant_score']) >= 70:\n",
        "    print(\"Relevant !!\")\n",
        "else:\n",
        "    print(\"Not Relevant !!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjVPLhGPaCXP",
        "outputId": "a0b4c8db-e04a-4c3d-bce4-3b60f13c2109"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relevance_check_result : 60\n",
            "Not Relevant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#def generate_answer(user_query):\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "generate_answer_system = \"\"\"\n",
        "당신은 친절하고 정확한 AI 어시스턴트입니다.\n",
        "\n",
        "아래의 문서(document)는 사용자의 질문(question)에 답변하기 위한 참고용 자료입니다.\n",
        "이 문서를 기반으로 질문에 대한 **정확하고 간결한 답변**을 생성하세요.\n",
        "\n",
        "- 문서 내용에 포함된 정보만을 기반으로 답변하세요.\n",
        "- 가능한 경우, 문서에서 근거가 되는 문장을 인용하거나 간단히 요약해서 포함하세요.\n",
        "\"\"\"\n",
        "\n",
        "generate_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", generate_answer_system),\n",
        "        (\"human\", \"question: {question}\\n\\n document: {document} \"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Chain\n",
        "rag_chain = generate_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run\n",
        "generation_answer = rag_chain.invoke({\"question\": user_query, \"document\": doc_txt})\n",
        "print(generation_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZb5r3sfan8M",
        "outputId": "ea6519cc-3c71-45c3-f5da-942e2b8883a2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A memory agent refers to a system that utilizes both short-term and long-term memory capabilities. Short-term memory allows the agent to learn from in-context information, while long-term memory enables it to retain and recall vast amounts of information over extended periods, often using an external vector store for fast retrieval. This combination enhances the agent's ability to function autonomously and effectively.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#def search_tavily():\n",
        "\n",
        "response = tavily.search(query=user_query, max_results=3)\n",
        "response.get('results')[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwn7m-9pcyR1",
        "outputId": "d93edf7e-72d3-44d5-860d-61c83608ddf9"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'what is memory agent ?', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'What Is AI Agent Memory? - ML Journey', 'url': 'https://mljourney.com/what-is-ai-agent-memory/', 'content': 'AI agent memory refers to the internal storage system that enables an intelligent agent to retain and access information across different points in time. This functionality is a critical leap beyond the capabilities of traditional prompt-based AI models, which handle each user input as an isolated event. Memory allows an agent to establish', 'score': 0.86981434, 'raw_content': None}, {'title': 'What Is AI Agent Memory? - IBM', 'url': 'https://www.ibm.com/think/topics/ai-agent-memory', 'content': 'What Is AI Agent Memory? What is AI agent memory? AI agent memory refers to an artificial intelligence\\xa0(AI) system’s ability to store and recall past experiences to improve decision-making, perception and overall performance. Long-term memory (LTM) allows AI agents to store and recall information across different sessions, making them more personalized and intelligent over time. AI agents typically implement semantic memory using knowledge bases, symbolic AI or vector embeddings, allowing them to process and retrieve relevant information efficiently. Frameworks for agentic AI memory LangGraph allows developers to construct hierarchical memory graphs for AI agents, improving their ability to track dependencies and learn over time. IBM AI agent development\\xa0', 'score': 0.8449346, 'raw_content': None}, {'title': 'Agent Memory Explained: What It Is and Why AI Agents Need It', 'url': 'https://www.gocodeo.com/post/agent-memory-explained-what-it-is-and-why-ai-agents-need-it', 'content': \"Agent memory is the bedrock of coherence, personalization, and reasoning. Whether you're developing AI copilots, customer support agents, AI project managers, or virtual tutors, memory is what will separate a great product from a mediocre one.\", 'score': 0.83290404, 'raw_content': None}], 'response_time': 0.99}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hallucination Checker\n",
        "\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "hallucination_checker_system = \"\"\"\n",
        "당신은 AI 모델이 생성한 답변이 주어진 문서(document)를 기반으로 한 것인지 검증하는 평가자입니다.\n",
        "\n",
        "\n",
        "- 문서(document)의 정보에 **명확히 근거한 경우에만** \"hallucinated\": false로 판단하세요.\n",
        "- 문서에 명확한 근거가 없거나, 과장/왜곡/추측이 섞인 경우는 \"hallucinated\": true로 판단하세요.\n",
        "- 문서와 전혀 관련 없는 정보가 포함된 경우도 \"hallucinated\": true입니다.\n",
        "\n",
        "생성한 답변:\n",
        "{answer}\n",
        "\n",
        "인용한 문서:\n",
        "{document}\n",
        "\n",
        "답변과 문서를 비교하여, 다음 형식의 JSON으로 판단 결과만 출력하세요:\n",
        "\n",
        "{{\n",
        "  \"hallucinated\": true or false\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "hallucination_checker_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", hallucination_checker_system),\n",
        "        (\"human\", \"answer: {answer}\\n\\n document: {document}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "hallucination_checker = hallucination_checker_prompt | llm | JsonOutputParser()\n",
        "hallucination_checker_result = hallucination_checker.invoke({\"answer\": generation_answer, \"document\": doc_txt})"
      ],
      "metadata": {
        "id": "_Cnsf2_VdL0F"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"hallucination_checker_result :\", hallucination_checker_result.get('hallucinated'))\n",
        "\n",
        "if hallucination_checker_result.get('hallucinated') == False:\n",
        "    print(\"Not Hallucinated !!\")\n",
        "    # Anser to User\n",
        "else:\n",
        "    print(\"Hallucinated !!\")\n",
        "    # Back to Generate Anser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hbt2ISgzdlPj",
        "outputId": "f75815f7-0fb0-4a03-a8c0-5af4f1438a9b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hallucination_checker_result : False\n",
            "Not Hallucinated !!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## START"
      ],
      "metadata": {
        "id": "AAM6p8HC5scu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain_community langchainhub chromadb langchain langgraph tavily-python langchain-text-splitters langchain_openai"
      ],
      "metadata": {
        "id": "r_bLYoad5Agx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature = 0)\n",
        "\n",
        "from tavily import TavilyClient\n",
        "tavily = TavilyClient(api_key='')\n",
        "\n",
        "### Index\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=250, chunk_overlap=0\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "# Add to vectorDB\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag-chroma\",\n",
        "    embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        ")\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Relevance Checker\n",
        "\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "relevance_checker_system = \"\"\"\n",
        "당신은 AI 기반 문서 연관성 평가자입니다.\n",
        "\n",
        "사용자의 질문과 검색된 문서 조각(context)을 참고하여, 이 문서가 질문에 **직접적인 관련이 있는지** 판단하세요.\n",
        "- 관련성이 높다면 relevant_score를 높은 점수로 설정하세요.\n",
        "- 관련성이 낮다면 relevant_score를 낮은 점수로 설정하세요.\n",
        "- relevant_score의 범위는 0 ~ 100 사이로 설정하세요.\n",
        "\n",
        "추측하지 말고, 문서 안에 **질문에 답할 수 있는 정보가 실제로 존재**할 때만 true를 반환하세요.\n",
        "\n",
        "질문:\n",
        "{question}\n",
        "\n",
        "문서 조각:\n",
        "{document}\n",
        "\n",
        "결과는 다음 형식의 JSON으로 반환하세요.\n",
        "{{\n",
        "  \"relevant_score\": 0 ~ 100\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "relevance_checker_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", relevance_checker_system),\n",
        "        (\"human\", \"question: {question}\\n\\n document: {document}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "relevance_checker = relevance_checker_prompt | llm | JsonOutputParser()\n",
        "\n",
        "#def generate_answer(user_query):\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "generate_answer_system = \"\"\"\n",
        "당신은 친절하고 정확한 AI 어시스턴트입니다.\n",
        "\n",
        "아래의 문서(document)는 사용자의 질문(question)에 답변하기 위한 참고용 자료입니다.\n",
        "이 문서를 기반으로 질문에 대한 **정확하고 간결한 답변**을 생성하세요.\n",
        "\n",
        "- 문서 내용에 포함된 정보만을 기반으로 답변하세요.\n",
        "- 가능한 경우, 문서에서 근거가 되는 문장을 인용하거나 간단히 요약해서 포함하세요.\n",
        "\"\"\"\n",
        "\n",
        "generate_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", generate_answer_system),\n",
        "        (\"human\", \"question: {question}\\n\\n document: {document} \"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Chain\n",
        "rag_chain = generate_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Hallucination Checker\n",
        "\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "hallucination_checker_system = \"\"\"\n",
        "당신은 AI 모델이 생성한 답변이 주어진 문서(document)를 기반으로 한 것인지 검증하는 평가자입니다.\n",
        "\n",
        "\n",
        "- 문서(document)의 정보에 **명확히 근거한 경우에만** \"hallucinated\": false로 판단하세요.\n",
        "- 문서에 명확한 근거가 없거나, 과장/왜곡/추측이 섞인 경우는 \"hallucinated\": true로 판단하세요.\n",
        "- 문서와 전혀 관련 없는 정보가 포함된 경우도 \"hallucinated\": true입니다.\n",
        "\n",
        "생성한 답변:\n",
        "{answer}\n",
        "\n",
        "인용한 문서:\n",
        "{document}\n",
        "\n",
        "답변과 문서를 비교하여, 다음 형식의 JSON으로 판단 결과만 출력하세요:\n",
        "\n",
        "{{\n",
        "  \"hallucinated\": true or false\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "hallucination_checker_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", hallucination_checker_system),\n",
        "        (\"human\", \"answer: {answer}\\n\\n document: {document}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "hallucination_checker = hallucination_checker_prompt | llm | JsonOutputParser()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAoeWryL2gYB",
        "outputId": "4f35f484-bc11-4805-8ca5-3304baaf5874"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "from typing import List\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "MAX_GENERATION_CNT = 1\n",
        "MAX_SEARCH_CNT = 1\n",
        "RELEVANT_SCORE_THRESHOLD = 50\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        question: question\n",
        "        generation: LLM generation\n",
        "        web_search: whether to add search\n",
        "        documents: list of documents\n",
        "    \"\"\"\n",
        "\n",
        "    question: str\n",
        "    documents: List[str]\n",
        "    ref_doc: str\n",
        "    generation: str\n",
        "    search_cnt: int\n",
        "    generation_cnt: int\n",
        "    stop_flag: bool\n",
        "\n",
        "def retrieve(state):\n",
        "  print(\"------------------------------ RETRIEVE ------------------------------\")\n",
        "\n",
        "  question = state[\"question\"]\n",
        "  documents = retriever.invoke(question)\n",
        "\n",
        "  print(\"question :\", question)\n",
        "  print(\"documents :\", documents)\n",
        "\n",
        "  return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "def web_search(state):\n",
        "  print(\"------------------------------ WEB SEARCH ------------------------------\")\n",
        "\n",
        "  print(\"now search_cnt: \", state[\"search_cnt\"])\n",
        "  if state[\"search_cnt\"] > MAX_SEARCH_CNT:\n",
        "    print(\"TOO_MANY_SEARCH !!! END ...\")\n",
        "    return {\"generation\": \"TOO_MANY_SEARCH !!! END ...\", \"stop_flag\": True}\n",
        "\n",
        "  question = state[\"question\"]\n",
        "\n",
        "  response = tavily.search(query=question, max_results=3)\n",
        "  # print(\"tavily search result :\", response.get('results'))\n",
        "\n",
        "  return {\"documents\": response.get('results'), \"search_cnt\" : state[\"search_cnt\"] + 1}\n",
        "\n",
        "def grade_documents(state):\n",
        "  print(\"------------------------------ CHECK DOCUMENT RELEVANCE TO QUESTION ------------------------------\")\n",
        "  question = state[\"question\"]\n",
        "  documents = state[\"documents\"]\n",
        "\n",
        "  for doc in documents:\n",
        "    relevance_checker_result = relevance_checker.invoke({\"question\": question, \"document\": doc})\n",
        "    print(\"relevance_check_score :\", relevance_checker_result['relevant_score'])\n",
        "    if relevance_checker_result['relevant_score'] >= RELEVANT_SCORE_THRESHOLD:\n",
        "      ref_doc = doc\n",
        "      return {\"ref_doc\": ref_doc}\n",
        "\n",
        "  return {\"ref_doc\": \"**NOT_RELEVANCE**\"}\n",
        "\n",
        "def generate(state):\n",
        "  print(\"------------------------------ GENERATE ANSWER ------------------------------\")\n",
        "\n",
        "  print(\"now generation_cnt: \", state[\"generation_cnt\"])\n",
        "  if state[\"generation_cnt\"] > MAX_GENERATION_CNT:\n",
        "    print(\"TOO_MANY_GENERATION !!! END ...\")\n",
        "    return {\"generation\": \"TOO_MANY_GENERATION !!! END ...\", \"stop_flag\": True}\n",
        "\n",
        "\n",
        "  question = state[\"question\"]\n",
        "  ref_doc = state[\"ref_doc\"]\n",
        "\n",
        "  generation_answer = rag_chain.invoke({\"question\": question, \"document\": ref_doc})\n",
        "\n",
        "  return {\"generation\": generation_answer, \"generation_cnt\" : state[\"generation_cnt\"] + 1}\n",
        "\n",
        "def hallucination_check(state):\n",
        "  print(\"------------------------------ CHECK HALLUCINATION ------------------------------\")\n",
        "\n",
        "  if state[\"stop_flag\"] :\n",
        "    return \"***END***\"\n",
        "\n",
        "  generation_answer = state[\"generation\"]\n",
        "\n",
        "  question = state[\"generation\"]\n",
        "\n",
        "  doc_txt = state[\"ref_doc\"]\n",
        "\n",
        "  hallucination_checker_result = hallucination_checker.invoke({\"answer\": generation_answer, \"document\": doc_txt})\n",
        "  print(\"hallucination_checker_result :\", hallucination_checker_result.get('hallucinated'))\n",
        "\n",
        "  if str(hallucination_checker_result.get('hallucinated')).lower() == \"false\":\n",
        "    print(\"NOT_HALLUCINATED, ANSWER RETURN\")\n",
        "    return \"NOT_HALLUCINATED\"\n",
        "  elif str(hallucination_checker_result.get('hallucinated')).lower() == \"true\":\n",
        "    print(\"'HALLUCINATED, NEED RE GENERATION\")\n",
        "    return \"HALLUCINATED\"\n",
        "\n",
        "def decide_to_generate(state):\n",
        "  print(\"------------------------------ DECIDE TO GENERATE ------------------------------\")\n",
        "\n",
        "  if state[\"stop_flag\"] :\n",
        "    return \"***END***\"\n",
        "\n",
        "  if state.get(\"ref_doc\") == \"**NOT_RELEVANCE**\":\n",
        "    print(\"GO TO WEB_SEARCH\")\n",
        "    return \"WEB_SEARCH\"\n",
        "  else:\n",
        "    print(\"GO TO GENERATE\")\n",
        "    return \"GENERATE\"\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "workflow.add_node(\"websearch\", web_search)  # web search\n",
        "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
        "workflow.add_node(\"generate\", generate)  # generatae\n",
        "workflow.add_node(\"hallucination_check\", hallucination_check)  # generate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fuWYSUkeIUn",
        "outputId": "5905a310-8dfe-449e-b580-020ab88f865d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7f6032cfb250>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 그래프 시작 지점\n",
        "workflow.set_entry_point(\"retrieve\")\n",
        "\n",
        "# 엣지 추가\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")      # 검색 → 관련성 평가\n",
        "\n",
        "# 조건부 엣지\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        \"GENERATE\": \"generate\",     # 충분히 관련 → 바로 답변 생성\n",
        "        \"WEB_SEARCH\": \"websearch\",  # 부족하면 → 웹 검색으로 보강\"\n",
        "        \"***END***\": END           # 종료\n",
        "    },\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"websearch\", \"grade_documents\")     # 웹 검색 후 → 관련성 평가\n",
        "\n",
        "## 환각 검증 결과에 따라 분기\n",
        "workflow.add_conditional_edges(\n",
        "    \"generate\",\n",
        "    hallucination_check,\n",
        "    {\n",
        "        \"NOT_HALLUCINATED\": END,     # 문제없으면 종료\n",
        "        \"HALLUCINATED\": \"generate\",  # 환각이면 추가 검색 후 재생성\n",
        "        \"***END***\": END           # 종료\n",
        "    },\n",
        ")\n",
        "\n",
        "# 그래프 컴파일\n",
        "graph = workflow.compile()"
      ],
      "metadata": {
        "id": "8XTi2Ol6iV6n"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 실행 예시\n",
        "result = graph.invoke({\"question\": \"에이전트가 뭔지 한글로 대답해.\", \"search_cnt\": 0, \"generation_cnt\": 0, \"stop_flag\": False})\n",
        "print(\"\\n\\n================ result ================\")\n",
        "pprint(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2HV-8vfjiqx",
        "outputId": "5ccba6b1-62a6-499d-f926-3d0b7dcece2a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------ RETRIEVE ------------------------------\n",
            "question : 에이전트가 뭔지 한글로 대답해.\n",
            "documents : [Document(metadata={'language': 'en', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='\"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"'), Document(metadata={'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='},\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:'), Document(metadata={'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'language': 'en', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.'}, page_content=\"Nlp\\nLanguage-Model\\nAgent\\nSteerability\\nPrompting\\n\\n\\n\\n« \\n\\nAdversarial Attacks on LLMs\\n\\n\\n »\\n\\nPrompt Engineering\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2025 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\"), Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so')]\n",
            "------------------------------ CHECK DOCUMENT RELEVANCE TO QUESTION ------------------------------\n",
            "relevance_check_score : 80\n",
            "------------------------------ DECIDE TO GENERATE ------------------------------\n",
            "GO TO GENERATE\n",
            "------------------------------ GENERATE ANSWER ------------------------------\n",
            "now generation_cnt:  0\n",
            "------------------------------ CHECK HALLUCINATION ------------------------------\n",
            "hallucination_checker_result : False\n",
            "NOT_HALLUCINATED, ANSWER RETURN\n",
            "\n",
            "\n",
            "================ result ================\n",
            "{'documents': [Document(metadata={'language': 'en', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='\"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"'),\n",
            "               Document(metadata={'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='},\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:'),\n",
            "               Document(metadata={'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'language': 'en', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.'}, page_content=\"Nlp\\nLanguage-Model\\nAgent\\nSteerability\\nPrompting\\n\\n\\n\\n« \\n\\nAdversarial Attacks on LLMs\\n\\n\\n »\\n\\nPrompt Engineering\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2025 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\"),\n",
            "               Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so')],\n",
            " 'generation': '에이전트는 LLM(대형 언어 모델)을 기반으로 한 자율 시스템으로, 여러 핵심 구성 요소를 포함합니다. '\n",
            "               '에이전트는 복잡한 작업을 작은 하위 목표로 나누어 효율적으로 처리하고, 과거 행동에 대한 반성과 개선을 통해 '\n",
            "               '결과의 질을 향상시킵니다. 또한, 외부 API를 호출하여 모델의 정보가 부족할 때 추가 정보를 얻는 기능도 '\n",
            "               '갖추고 있습니다.',\n",
            " 'generation_cnt': 1,\n",
            " 'question': '에이전트가 뭔지 한글로 대답해.',\n",
            " 'ref_doc': Document(metadata={'language': 'en', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='\"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"'),\n",
            " 'search_cnt': 0,\n",
            " 'stop_flag': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 실행 예시\n",
        "result = graph.invoke({\"question\": \"대한민국 서울의 오늘 날씨 알려줘.\", \"search_cnt\": 0, \"generation_cnt\": 0, \"stop_flag\": False})\n",
        "print(\"\\n\\n================ result ================\")\n",
        "pprint(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bHumxEBpskk",
        "outputId": "866a92cf-7f0c-41f8-82e4-e911448effa0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------ RETRIEVE ------------------------------\n",
            "question : 대한민국 서울의 오늘 날씨 알려줘.\n",
            "documents : [Document(metadata={'language': 'en', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"'), Document(metadata={'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.'), Document(metadata={'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.'}, page_content='\"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that'), Document(metadata={'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\"}, page_content=\"Text: i'll bet the video game is a lot more fun than the film. \\nSentiment:\\nExplaining the desired audience is another smart way to give instructions\")]\n",
            "------------------------------ CHECK DOCUMENT RELEVANCE TO QUESTION ------------------------------\n",
            "relevance_check_score : 0\n",
            "relevance_check_score : 0\n",
            "relevance_check_score : 0\n",
            "relevance_check_score : 0\n",
            "------------------------------ DECIDE TO GENERATE ------------------------------\n",
            "GO TO WEB_SEARCH\n",
            "------------------------------ WEB SEARCH ------------------------------\n",
            "now search_cnt:  0\n",
            "------------------------------ CHECK DOCUMENT RELEVANCE TO QUESTION ------------------------------\n",
            "relevance_check_score : 90\n",
            "------------------------------ DECIDE TO GENERATE ------------------------------\n",
            "GO TO GENERATE\n",
            "------------------------------ GENERATE ANSWER ------------------------------\n",
            "now generation_cnt:  0\n",
            "------------------------------ CHECK HALLUCINATION ------------------------------\n",
            "hallucination_checker_result : False\n",
            "NOT_HALLUCINATED, ANSWER RETURN\n",
            "\n",
            "\n",
            "================ result ================\n",
            "{'documents': [{'content': '서울특별시, 서울시, 대한민국 시간별 날씨 | AccuWeather 서울특별시, 서울시 '\n",
            "                           '========== 65°F 현재 위치 사용 서울특별시 서울시 65° 서울특별시, 서울시 '\n",
            "                           '날씨 오늘WinterCast지역 {stormName} '\n",
            "                           '추적기시간별일별레이더MinuteCast월대기질건강 및 활동 전 세계 ### 허리케인### '\n",
            "                           '악천후 기상### 레이더 및 지도### 동영상 오늘시간별 '\n",
            "                           '---일별레이더MinuteCast월대기질건강 및 활동 64° RealFeel® 65°  '\n",
            "                           '맑음 바람 서남서 4mi/h 돌풍 9mi/h 가시거리 10mi 62° RealFeel® '\n",
            "                           '62°  대체로 맑음 바람 남서 4mi/h 돌풍 7mi/h 가시거리 10mi 59° '\n",
            "                           'RealFeel® 60°  바람 남서 2mi/h 돌풍 7mi/h 가시거리 10mi 전 세계 '\n",
            "                           '### 허리케인### 악천후 기상### 레이더 및 지도### 동영상 전 '\n",
            "                           '세계아시아대한민국서울시서울특별시 동작구, 서울시 마포구, 서울시 서대문구, 서울시 이용 '\n",
            "                           '약관 | 개인정보 보호정책 | 쿠키 사용 정책|개인정보 보호 정보 내 개인 정보를 '\n",
            "                           '판매하거나 공유하지 마십시오. Get AccuWeather alerts as they '\n",
            "                           'happen with our browser notifications. Enable '\n",
            "                           'Notifications Notifications Enabled',\n",
            "                'raw_content': None,\n",
            "                'score': 0.75852895,\n",
            "                'title': '서울특별시, 서울시, 대한민국 시간별 날씨 - AccuWeather',\n",
            "                'url': 'https://www.accuweather.com/ko/kr/seoul/226081/hourly-weather-forecast/226081'},\n",
            "               {'content': 'Refresh Page 구름 많음 기온 시간별 예보 오전 11시 69° 0%오후 12시 '\n",
            "                           '72° 0%오후 1시 75° 0%오후 2시 76° 0%오후 3시 76° 0%오후 4시 '\n",
            "                           '77° 0%오후 5시 75° 0%오후 6시 73° 0%오후 7시 71° 0%오후 8시 '\n",
            "                           '68° 0%오후 9시 65° 0%오후 10시 63° 0% 일별 예보 오늘 5. 77° '\n",
            "                           '55° 흐릿함 약간 흐림 0%화 5. 80° 60° 흐릿함 대체로 흐림; 밤 늦게 때때로 '\n",
            "                           '강한 뇌우가 내림 1%목 5. 70° 59° 오전에 거센 소나기; 대체로 흐림 대체로 흐림 '\n",
            "                           '81%금 5.',\n",
            "                'raw_content': None,\n",
            "                'score': 0.751375,\n",
            "                'title': '서울특별시, 서울시, 대한민국 3일 날씨 예보 - AccuWeather',\n",
            "                'url': 'https://www.accuweather.com/ko/kr/seoul/226081/weather-forecast/226081'},\n",
            "               {'content': '날씨 바다 영상·일기도 태풍 기후 기후변화 지진·화산 테마날씨 황사 관측 소식·지식 개편된 '\n",
            "                           '날씨누리 홈페이지 첫 화면에서 지도를 통한 실시간 기상정보와 레이더영상, 전국 특보 현황을 '\n",
            "                           '조회할 수 있으며, 내 지역의 현재 기상상황과 대기질 상태, 일출/일몰 시각을 확인할 수 '\n",
            "                           \"있습니다. 바다날씨를 조회하고자 하는 경우, '날씨누리 바다>바다예보>일일예보' 페이지에서 \"\n",
            "                           '해역을 선택하여 해역별 바다날씨를 확인할 수 있습니다. 현재 위치 설정 혹은 검색을 통해 '\n",
            "                           '조회한 육상지역 및 해역은 위치 정보 우측의 별표를 클릭하여 관심지역으로 설정할 수 '\n",
            "                           '있으며, 관심지역 설정 해제 또한 별표를 다시 클릭하여 해제할 수 있습니다. 설정한 '\n",
            "                           '관심지역은 현재 위치 정보 우측의 화살표를 클릭하여 확인할 수 있으며, 사용중인 브라우저 '\n",
            "                           '캐시 영역에 저장되므로 캐시 삭제시 설정한 관심지역 정보가 초기화됩니다.\" 날씨누리 '\n",
            "                           '첫화면에서 육상 및 해상의 현재 기상정보, 초단기 강수예측, 특보 상황을 지도상에서 확인할 '\n",
            "                           '수 있습니다.',\n",
            "                'raw_content': None,\n",
            "                'score': 0.34707275,\n",
            "                'title': '서울 ·경기도 중기예보 - 기상청 날씨누리',\n",
            "                'url': 'https://www.weather.go.kr/w/weather/forecast/mid-term.do?stnId1=109'}],\n",
            " 'generation': '오늘 서울의 날씨는 맑고 기온은 약 18도(65°F)입니다. 바람은 서남서 방향에서 시속 4마일로 불고 있으며, '\n",
            "               '가시거리는 10마일입니다.',\n",
            " 'generation_cnt': 1,\n",
            " 'question': '대한민국 서울의 오늘 날씨 알려줘.',\n",
            " 'ref_doc': {'content': '서울특별시, 서울시, 대한민국 시간별 날씨 | AccuWeather 서울특별시, 서울시 '\n",
            "                        '========== 65°F 현재 위치 사용 서울특별시 서울시 65° 서울특별시, 서울시 날씨 '\n",
            "                        '오늘WinterCast지역 {stormName} '\n",
            "                        '추적기시간별일별레이더MinuteCast월대기질건강 및 활동 전 세계 ### 허리케인### 악천후 '\n",
            "                        '기상### 레이더 및 지도### 동영상 오늘시간별 ---일별레이더MinuteCast월대기질건강 '\n",
            "                        '및 활동 64° RealFeel® 65°  맑음 바람 서남서 4mi/h 돌풍 9mi/h 가시거리 '\n",
            "                        '10mi 62° RealFeel® 62°  대체로 맑음 바람 남서 4mi/h 돌풍 7mi/h '\n",
            "                        '가시거리 10mi 59° RealFeel® 60°  바람 남서 2mi/h 돌풍 7mi/h '\n",
            "                        '가시거리 10mi 전 세계 ### 허리케인### 악천후 기상### 레이더 및 지도### 동영상 '\n",
            "                        '전 세계아시아대한민국서울시서울특별시 동작구, 서울시 마포구, 서울시 서대문구, 서울시 이용 약관 '\n",
            "                        '| 개인정보 보호정책 | 쿠키 사용 정책|개인정보 보호 정보 내 개인 정보를 판매하거나 공유하지 '\n",
            "                        '마십시오. Get AccuWeather alerts as they happen with our '\n",
            "                        'browser notifications. Enable Notifications '\n",
            "                        'Notifications Enabled',\n",
            "             'raw_content': None,\n",
            "             'score': 0.75852895,\n",
            "             'title': '서울특별시, 서울시, 대한민국 시간별 날씨 - AccuWeather',\n",
            "             'url': 'https://www.accuweather.com/ko/kr/seoul/226081/hourly-weather-forecast/226081'},\n",
            " 'search_cnt': 1,\n",
            " 'stop_flag': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 실행 예시\n",
        "result = graph.invoke({\"question\": \"집에 가려면 어떻게 해 ? 다양한 비유를 섞여서 설명해.\", \"search_cnt\": 0, \"stop_flag\": False})\n",
        "print(\"\\n\\n================ result ================\")\n",
        "pprint(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZCyt5EGufmg",
        "outputId": "ad975812-c2cc-4d78-e7c7-5ad9b0d0cc4e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------ RETRIEVE ------------------------------\n",
            "question : 집에 가려면 어떻게 해 ? 다양한 비유를 섞여서 설명해.\n",
            "documents : [Document(metadata={'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:'), Document(metadata={'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.'}, page_content='\"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'language': 'en', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.'}, page_content='\"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that')]\n",
            "------------------------------ CHECK DOCUMENT RELEVANCE TO QUESTION ------------------------------\n",
            "relevance_check_score : 0\n",
            "relevance_check_score : 0\n",
            "relevance_check_score : 0\n",
            "relevance_check_score : 0\n",
            "------------------------------ DECIDE TO GENERATE ------------------------------\n",
            "GO TO WEB_SEARCH\n",
            "------------------------------ WEB SEARCH ------------------------------\n",
            "now search_cnt:  0\n",
            "------------------------------ CHECK DOCUMENT RELEVANCE TO QUESTION ------------------------------\n",
            "relevance_check_score : 0\n",
            "relevance_check_score : 0\n",
            "relevance_check_score : 0\n",
            "------------------------------ DECIDE TO GENERATE ------------------------------\n",
            "GO TO WEB_SEARCH\n",
            "------------------------------ WEB SEARCH ------------------------------\n",
            "now search_cnt:  1\n",
            "------------------------------ CHECK DOCUMENT RELEVANCE TO QUESTION ------------------------------\n",
            "relevance_check_score : 0\n",
            "relevance_check_score : 0\n",
            "relevance_check_score : 0\n",
            "------------------------------ DECIDE TO GENERATE ------------------------------\n",
            "GO TO WEB_SEARCH\n",
            "------------------------------ WEB SEARCH ------------------------------\n",
            "now search_cnt:  2\n",
            "TOO_MANY_SEARCH !!! END ...\n",
            "------------------------------ CHECK DOCUMENT RELEVANCE TO QUESTION ------------------------------\n",
            "relevance_check_score : 0\n",
            "relevance_check_score : 0\n",
            "relevance_check_score : 0\n",
            "------------------------------ DECIDE TO GENERATE ------------------------------\n",
            "\n",
            "\n",
            "================ result ================\n",
            "{'documents': [{'content': '다음은 김운희 교수의 \"대쥬신을 찾아서\"에 관한 신문기사 입니다. \\'쥬신\\'이란 게 '\n",
            "                           '뭔가. 숙신, 조선과 같은 뿌리를 지닌 쥬신이란 말은 간단히 말하자면,',\n",
            "                'raw_content': None,\n",
            "                'score': 0.030516557,\n",
            "                'title': '정리중 2 - 역사관련 재밌는 얘기들 - 한국사를사랑하는모임 - Daum 카페',\n",
            "                'url': 'https://cafe.daum.net/cjwhc/1nxV/7320'},\n",
            "               {'content': '도서명: 심리학개론 저자명: 조대경외 9명 공저 출판사명: 한국방송통신대학출판부 출판년도: '\n",
            "                           '1991년 묵자책의 페이지: 492.',\n",
            "                'raw_content': None,\n",
            "                'score': 0.023529684,\n",
            "                'title': '심리학개론 : 네이버 블로그',\n",
            "                'url': 'https://blog.naver.com/utimegps/70003065423?viewType=pc'},\n",
            "               {'content': '어려서부터 몸이 불편한 오빠를 보며 그런 오빠의 부담을 덜어주기 위해 자신이 오빠 대신 '\n",
            "                           '마이어즈 가의 적남이 되어야겠다고 결심해 남장을 시작했다.',\n",
            "                'raw_content': None,\n",
            "                'score': 0.019184623,\n",
            "                'title': '인피니트 덴드로그램/등장인물 (r3222 판) - 나무위키',\n",
            "                'url': 'https://namu.wiki/w/%EC%9D%B8%ED%94%BC%EB%8B%88%ED%8A%B8%20%EB%8D%B4%EB%93%9C%EB%A1%9C%EA%B7%B8%EB%9E%A8/%EB%93%B1%EC%9E%A5%EC%9D%B8%EB%AC%BC?uuid=c622209e-4381-4330-9ee3-c1ec191bd882'}],\n",
            " 'generation': 'TOO_MANY_SEARCH !!! END ...',\n",
            " 'question': '집에 가려면 어떻게 해 ? 다양한 비유를 섞여서 설명해.',\n",
            " 'ref_doc': '**NOT_RELEVANCE**',\n",
            " 'search_cnt': 2,\n",
            " 'stop_flag': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional #2"
      ],
      "metadata": {
        "id": "v6GgXoPO8q4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import ConfigurableFieldSpec\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You're an assistant who speaks in {language}. Respond in 20 words or fewer\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "runnable = prompt | llm\n",
        "\n",
        "def get_session_history(user_id: str, conversation_id: str):\n",
        "    return SQLChatMessageHistory(f\"{user_id}--{conversation_id}\", \"sqlite:///memory.db\")\n",
        "\n",
        "with_message_history = RunnableWithMessageHistory(\n",
        "    runnable,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"user_id\",\n",
        "            annotation=str,\n",
        "            name=\"User ID\",\n",
        "            description=\"Unique identifier for the user.\",\n",
        "            default=\"\",\n",
        "            is_shared=True,\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"conversation_id\",\n",
        "            annotation=str,\n",
        "            name=\"Conversation ID\",\n",
        "            description=\"Unique identifier for the conversation.\",\n",
        "            default=\"\",\n",
        "            is_shared=True,\n",
        "        ),\n",
        "    ],\n",
        ")\n",
        "\n",
        "with_message_history.invoke(\n",
        "    {\"language\": \"korean\", \"input\": \"hi im bob!\"},\n",
        "    config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}},\n",
        ")"
      ],
      "metadata": {
        "id": "pp4aPd3z8uIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tavily import TavilyClient\n",
        "tavily_test = TavilyClient(api_key='')\n",
        "\n",
        "response1 = tavily_test.search(query=\"Where does Messi play right now?\", max_results=3)\n",
        "context = [{\"url\": obj[\"url\"], \"content\": obj[\"content\"]} for obj in response1['results']]\n",
        "\n",
        "# You can easily get search result context based on any max tokens straight into your RAG.\n",
        "# The response is a string of the context within the max_token limit.\n",
        "\n",
        "response2 = tavily_test.get_search_context(query=\"Where does Messi play right now?\", search_depth=\"advanced\", max_tokens=500)\n",
        "\n",
        "# You can also get a simple answer to a question including relevant sources all with a simple function call:\n",
        "# You can use it for baseline\n",
        "response3 = tavily_test.qna_search(query=\"Where does Messi play right now?\")\n"
      ],
      "metadata": {
        "id": "U0qAs6bO9JQ4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"======================== 1 ========================\")\n",
        "print(response1)\n",
        "\n",
        "print(\"======================== 2 ========================\")\n",
        "print(response2)\n",
        "\n",
        "print(\"======================== 3 ========================\")\n",
        "print(response3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMB9AUFk9bBB",
        "outputId": "b93f7638-65d0-4b3b-ab16-dcc39721927d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================== 1 ========================\n",
            "{'query': 'Where does Messi play right now?', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': \"Where's Messi? Why Isn't He Playing For Inter Miami? Did He Leave?\", 'url': 'https://stylecaster.com/entertainment/celebrity-news/1638745/where-messi-inter-miami/', 'content': 'Messi has always been expected to miss some games with Inter Miami. Back in August 2023, Martino confirmed that the footballer would miss \"at least three games\" with the Miami team this season.', 'score': 0.6100127, 'raw_content': None}, {'title': 'Is Lionel Messi playing today? Status, lineup for next Inter Miami game ...', 'url': 'https://www.sportingnews.com/us/soccer/news/lionel-messi-playing-today-status-lineup-inter-miami-2025/b87bb697bffbfbd6b7de8a7a', 'content': \"Every week, Lionel Messi garners more attention than any other player in Major League Soccer, and the 37-year-old's fitness is a consistent talking point.\", 'score': 0.539833, 'raw_content': None}, {'title': 'Lionel Messi - Inter Miami CF Forward - ESPN', 'url': 'https://www.espn.com/soccer/player/_/id/45843/lionel-messi', 'content': 'View the profile of Inter Miami CF Forward Lionel Messi on ESPN. Get the latest news, live stats and game highlights.', 'score': 0.43943515, 'raw_content': None}], 'response_time': 1.78}\n",
            "======================== 2 ========================\n",
            "[{\"url\": \"https://www.si.com/soccer/lionel-messi-retirement-when-will-messi-retire-what-next-soccer-goat\", \"content\": \"Messi has already [revealed](https://www.marca.com/en/football/mls/2024/06/12/6669d8f2ca4741f90d8b456c.html) his final club will be Inter Miami, where his current contract is due to expire at the end of 2025. However, reports indicate that there are negotiations ongoing for Messi to [sign a new deal with the Herons](https://www.si.com/soccer/lionel-messi-and-inter-miami-close-to-contract-extension-per-report) for at least the 2026 MLS season, where Inter Miami will move to their new stadium. [...] This situation goes along with the prevailing thought that Messi will participate in the [2026 World Cup](https://www.si.com/soccer/will-lionel-messi-play-at-the-2026-world-cup), where Argentina will defend its champions crown. Prominent figures close to Messi such as teammate [Luis Su\\u00c3\\u00a1rez](https://www.espn.com/soccer/story/_/id/44681073/lionel-messi-2026-world-cup-luis-suarez-says-wants-play) and Argentina manager [Lionel [...] Yet, it's hard to envision Messi playing for La Albiceleste past 2026. He's made his desire to play another World Cup clear, but he's also acknowledged the need for new, young, up-and-coming players to take over the national team. This opens the door for Messi to retire from international soccer before he hangs up his boots for good.\\n\\n## What\\u00e2\\u0080\\u0099s Next for Lionel Messi After Retirement?\"}, {\"url\": \"https://en.wikipedia.org/wiki/Lionel_Messi\", \"content\": \"is an Argentine professional footballer who plays as a forward for and captains both Major League Soccer club Inter Miami and the Argentina national team.\"}]\n",
            "======================== 3 ========================\n",
            "Lionel Messi currently plays for Inter Miami in Major League Soccer. His contract is set to expire at the end of 2025. Negotiations for an extension are ongoing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5iwImZf9ktZ",
        "outputId": "4ac0bfb8-cded-4ebc-fedf-0726e9874f90"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.46.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.43.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.6.15)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gpt4all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8geJkUJzCw33",
        "outputId": "7a07f082-6b7e-4cad-eab8-96c44cd6099f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gpt4all\n",
            "  Downloading gpt4all-2.8.2-py3-none-manylinux1_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from gpt4all) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gpt4all) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (2025.6.15)\n",
            "Downloading gpt4all-2.8.2-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gpt4all\n",
            "Successfully installed gpt4all-2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st"
      ],
      "metadata": {
        "id": "oF8ZGaIuCL8X"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from tavily import TavilyClient\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langgraph.graph import END, StateGraph\n",
        "from typing import List\n",
        "from typing_extensions import TypedDict\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "tavily = TavilyClient(api_key=\"\")\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"Research Assistant\",\n",
        "    page_icon=\":orange_heart:\",\n",
        ")\n",
        "\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    question: str\n",
        "    generation: str\n",
        "    web_search: str\n",
        "    documents: List[Document]\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 모델 선택\n",
        "    llm_model = st.sidebar.selectbox(\n",
        "        \"Select Model\",\n",
        "        options=[\n",
        "            \"llama3\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    urls = [\n",
        "        \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "        \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "        \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "    ]\n",
        "\n",
        "    # 웹 페이지 로드 및 문서 분할\n",
        "    docs = [WebBaseLoader(url).load() for url in urls]\n",
        "    docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=250, chunk_overlap=0\n",
        "    )\n",
        "    doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "    # 벡터 저장소 생성\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=doc_splits,\n",
        "        collection_name=\"rag-chroma\",\n",
        "        embedding=GPT4AllEmbeddings(model_name=\"all-MiniLM-L6-v2.gguf2.f16.gguf\"),\n",
        "    )\n",
        "    retriever = vectorstore.as_retriever()\n",
        "\n",
        "    # RAG 에이전트 노드 및 엣지 정의\n",
        "    def retrieve(state):\n",
        "        print(\"---RETRIEVE---\")\n",
        "        question = state[\"question\"]\n",
        "        documents = retriever.invoke(question)\n",
        "        return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "    def generate(state):\n",
        "        print(\"---GENERATE---\")\n",
        "        question = state[\"question\"]\n",
        "        documents = state[\"documents\"]\n",
        "        generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "        return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "\n",
        "    def grade_documents(state):\n",
        "        print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
        "        question = state[\"question\"]\n",
        "        documents = state[\"documents\"]\n",
        "        filtered_docs = []\n",
        "        web_search = \"No\"\n",
        "        for d in documents:\n",
        "            score = retrieval_grader.invoke(\n",
        "                {\"question\": question, \"document\": d.page_content}\n",
        "            )\n",
        "            grade = score[\"score\"]\n",
        "            if grade.lower() == \"yes\":\n",
        "                print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "                filtered_docs.append(d)\n",
        "            else:\n",
        "                print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "                web_search = \"Yes\"\n",
        "                continue\n",
        "        return {\n",
        "            \"documents\": filtered_docs,\n",
        "            \"question\": question,\n",
        "            \"web_search\": web_search,\n",
        "        }\n",
        "\n",
        "    def web_search(state):\n",
        "        print(\"---WEB SEARCH---\")\n",
        "        question = state[\"question\"]\n",
        "        documents = state[\"documents\"]\n",
        "        docs = tavily.search(query=question)[\"results\"]\n",
        "        web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
        "        web_results = Document(page_content=web_results)\n",
        "        if documents is not None:\n",
        "            documents.append(web_results)\n",
        "        else:\n",
        "            documents = [web_results]\n",
        "        return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "    def route_question(state):\n",
        "        print(\"---ROUTE QUESTION---\")\n",
        "        question = state[\"question\"]\n",
        "        print(question)\n",
        "        source = question_router.invoke({\"question\": question})\n",
        "        print(source)\n",
        "        print(source[\"datasource\"])\n",
        "        if source[\"datasource\"] == \"web_search\":\n",
        "            print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
        "            return \"websearch\"\n",
        "        elif source[\"datasource\"] == \"vectorstore\":\n",
        "            print(\"---ROUTE QUESTION TO RAG---\")\n",
        "            return \"vectorstore\"\n",
        "\n",
        "    def decide_to_generate(state):\n",
        "        print(\"---ASSESS GRADED DOCUMENTS---\")\n",
        "        state[\"question\"]\n",
        "        web_search = state[\"web_search\"]\n",
        "        state[\"documents\"]\n",
        "\n",
        "        if web_search == \"Yes\":\n",
        "            print(\n",
        "                \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n",
        "            )\n",
        "            return \"websearch\"\n",
        "        else:\n",
        "            print(\"---DECISION: GENERATE---\")\n",
        "            return \"generate\"\n",
        "\n",
        "    def grade_generation_v_documents_and_question(state):\n",
        "        print(\"---CHECK HALLUCINATIONS---\")\n",
        "        question = state[\"question\"]\n",
        "        documents = state[\"documents\"]\n",
        "        generation = state[\"generation\"]\n",
        "\n",
        "        score = hallucination_grader.invoke(\n",
        "            {\"documents\": documents, \"generation\": generation}\n",
        "        )\n",
        "        grade = score[\"score\"]\n",
        "\n",
        "        if grade == \"yes\":\n",
        "            print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
        "            print(\"---GRADE GENERATION vs QUESTION---\")\n",
        "            score = answer_grader.invoke(\n",
        "                {\"question\": question, \"generation\": generation}\n",
        "            )\n",
        "            grade = score[\"score\"]\n",
        "            if grade == \"yes\":\n",
        "                print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
        "                return \"useful\"\n",
        "            else:\n",
        "                print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
        "                return \"not useful\"\n",
        "        else:\n",
        "            print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
        "            return \"not supported\"\n",
        "\n",
        "    # RAG 에이전트 그래프 구성\n",
        "    workflow = StateGraph(GraphState)\n",
        "    workflow.add_node(\"websearch\", web_search)\n",
        "    workflow.add_node(\"retrieve\", retrieve)\n",
        "    workflow.add_node(\"grade_documents\", grade_documents)\n",
        "    workflow.add_node(\"generate\", generate)\n",
        "\n",
        "    workflow.set_conditional_entry_point(\n",
        "        route_question,\n",
        "        {\n",
        "            \"websearch\": \"websearch\",\n",
        "            \"vectorstore\": \"retrieve\",\n",
        "        },\n",
        "    )\n",
        "\n",
        "    workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "    workflow.add_conditional_edges(\n",
        "        \"grade_documents\",\n",
        "        decide_to_generate,\n",
        "        {\n",
        "            \"websearch\": \"websearch\",\n",
        "            \"generate\": \"generate\",\n",
        "        },\n",
        "    )\n",
        "    workflow.add_edge(\"websearch\", \"generate\")\n",
        "    workflow.add_conditional_edges(\n",
        "        \"generate\",\n",
        "        grade_generation_v_documents_and_question,\n",
        "        {\n",
        "            \"not supported\": \"generate\",\n",
        "            \"useful\": END,\n",
        "            \"not useful\": \"websearch\",\n",
        "        },\n",
        "    )\n",
        "\n",
        "    app = workflow.compile()\n",
        "\n",
        "    # rag_chain 정의\n",
        "    llm = ChatOllama(model=llm_model, temperature=0)\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks.\n",
        "        Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
        "        Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "        Question: {question}\n",
        "        Context: {context}\n",
        "        Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
        "        input_variables=[\"question\", \"context\"],\n",
        "    )\n",
        "    rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "    # retrieval_grader, hallucination_grader, answer_grader 정의\n",
        "    llm = ChatOllama(model=llm_model, format=\"json\", temperature=0)\n",
        "\n",
        "    retrieval_grader_prompt = PromptTemplate(\n",
        "        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance\n",
        "        of a retrieved document to a user question. If the document contains keywords related to the user question,\n",
        "        grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
        "        Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
        "         <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "        Here is the retrieved document: \\n\\n {document} \\n\\n\n",
        "        Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "        \"\"\",\n",
        "        input_variables=[\"question\", \"document\"],\n",
        "    )\n",
        "    retrieval_grader = retrieval_grader_prompt | llm | JsonOutputParser()\n",
        "\n",
        "    hallucination_grader_prompt = PromptTemplate(\n",
        "        template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether\n",
        "        an answer is grounded in / supported by a set of facts. Give a binary 'yes' or 'no' score to indicate\n",
        "        whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a\n",
        "        single key 'score' and no preamble or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "        Here are the facts:\n",
        "        \\n ------- \\n\n",
        "        {documents}\n",
        "        \\n ------- \\n\n",
        "        Here is the answer: {generation}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
        "        input_variables=[\"generation\", \"documents\"],\n",
        "    )\n",
        "    hallucination_grader = hallucination_grader_prompt | llm | JsonOutputParser()\n",
        "\n",
        "    answer_grader_prompt = PromptTemplate(\n",
        "        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether an\n",
        "        answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is\n",
        "        useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
        "         <|eot_id|><|start_header_id|>user<|end_header_id|> Here is the answer:\n",
        "        \\n ------- \\n\n",
        "        {generation}\n",
        "        \\n ------- \\n\n",
        "        Here is the question: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
        "        input_variables=[\"generation\", \"question\"],\n",
        "    )\n",
        "    answer_grader = answer_grader_prompt | llm | JsonOutputParser()\n",
        "\n",
        "    question_router_prompt = PromptTemplate(\n",
        "        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an expert at routing a\n",
        "    user question to a vectorstore or web search. Use the vectorstore for questions on LLM  agents,\n",
        "    prompt engineering, and adversarial attacks. You do not need to be stringent with the keywords\n",
        "    in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search'\n",
        "    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and\n",
        "    no premable or explanation. Question to route: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
        "        input_variables=[\"question\"],\n",
        "    )\n",
        "    question_router = question_router_prompt | llm | JsonOutputParser()\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # Streamlit 앱 UI\n",
        "    st.title(\"Research Assistant powered by OpenAI\")\n",
        "\n",
        "    input_topic = st.text_input(\n",
        "        \":female-scientist: Enter a topic\",\n",
        "        value=\"Superfast Llama 3 inference on Groq Cloud\",\n",
        "    )\n",
        "\n",
        "    generate_report = st.button(\"Generate Report\")\n",
        "\n",
        "    if generate_report:\n",
        "        with st.spinner(\"Generating Report\"):\n",
        "            inputs = {\"question\": input_topic}\n",
        "            for output in app.stream(inputs):\n",
        "                for key, value in output.items():\n",
        "                    print(f\"Finished running: {key}:\")\n",
        "            final_report = value[\"generation\"]\n",
        "            st.markdown(final_report)\n",
        "\n",
        "    st.sidebar.markdown(\"---\")\n",
        "    if st.sidebar.button(\"Restart\"):\n",
        "        st.session_state.clear()\n",
        "        st.experimental_rerun()\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        },
        "id": "ymERfOmQCQDK",
        "outputId": "9355e30f-68ef-4c42-db57-2df24b0426e0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-06-25 07:42:51.639 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-25 07:42:51.644 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-25 07:42:51.645 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-25 07:42:51.646 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-25 07:42:51.647 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-25 07:42:51.649 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-25 07:42:51.649 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-25 07:42:51.650 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Could not import gpt4all library. Please install the gpt4all library to use this embedding model: pip install gpt4all",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/embeddings/gpt4all.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgpt4all\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbed4All\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gpt4all'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-17-2216947269.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-17-2216947269.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoc_splits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mcollection_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rag-chroma\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGPT4AllEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"all-MiniLM-L6-v2.gguf2.f16.gguf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     )\n\u001b[1;32m     60\u001b[0m     \u001b[0mretriever\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_retriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/embeddings/gpt4all.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m     45\u001b[0m             )\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0;34m\"Could not import gpt4all library. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;34m\"Please install the gpt4all library to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Could not import gpt4all library. Please install the gpt4all library to use this embedding model: pip install gpt4all",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mp4bgG1TCVvT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}